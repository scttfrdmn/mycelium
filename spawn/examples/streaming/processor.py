#!/usr/bin/env python3
"""
Example streaming data processor for Spawn pipelines.

This processor receives data from an upstream producer, processes it,
and forwards to downstream consumers. Demonstrates fan-in and fan-out
patterns using the peer discovery file.
"""

import argparse
import json
import socket
import struct
import threading
import time
from queue import Queue
from typing import List


def load_peer_discovery():
    """Load the peer discovery file generated by Spawn."""
    with open('/etc/spawn/pipeline-peers.json', 'r') as f:
        return json.load(f)


def recv_chunk(sock: socket.socket) -> bytes:
    """Receive a length-prefixed chunk from TCP."""
    # Read 4-byte length prefix
    length_data = recv_exactly(sock, 4)
    if not length_data:
        return None

    length = struct.unpack('>I', length_data)[0]

    # Sanity check
    if length > 100 * 1024 * 1024:  # 100MB max
        raise ValueError(f"Chunk too large: {length} bytes")

    # Read data
    return recv_exactly(sock, length)


def recv_exactly(sock: socket.socket, n: int) -> bytes:
    """Receive exactly n bytes from socket."""
    data = b''
    while len(data) < n:
        chunk = sock.recv(n - len(data))
        if not chunk:
            return None
        data += chunk
    return data


def send_chunk(sock: socket.socket, data: bytes) -> None:
    """Send a length-prefixed chunk over TCP."""
    length = len(data)
    sock.sendall(struct.pack('>I', length))
    sock.sendall(data)


def process_batch(batch_data: bytes) -> bytes:
    """Process a batch of data."""
    lines = batch_data.decode('utf-8').strip().split('\n')
    processed_lines = []

    for line in lines:
        if not line:
            continue

        parts = line.split(',')
        if len(parts) != 3:
            continue

        record_id, value, timestamp = parts
        # Apply some processing (e.g., multiply by 2)
        new_value = float(value) * 2.0
        processed_lines.append(f"{record_id},{new_value},{timestamp}")

    return '\n'.join(processed_lines).encode('utf-8')


def upstream_receiver(upstream_ip: str, upstream_port: int, queue: Queue):
    """Thread that receives data from upstream producer."""
    print(f"Connecting to upstream producer at {upstream_ip}:{upstream_port}")

    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.connect((upstream_ip, upstream_port))
    print(f"Connected to upstream producer")

    batch_count = 0
    while True:
        try:
            chunk = recv_chunk(sock)
            if not chunk:
                print("Upstream connection closed")
                break

            batch_count += 1
            queue.put(chunk)

            if batch_count % 10 == 0:
                print(f"Received {batch_count} batches from upstream")

        except Exception as e:
            print(f"Error receiving from upstream: {e}")
            break

    sock.close()
    queue.put(None)  # Signal end of stream


def downstream_forwarder(connections: List[socket.socket], queue: Queue):
    """Thread that forwards processed data to downstream consumers."""
    batch_count = 0

    while True:
        chunk = queue.get()
        if chunk is None:
            break

        # Process the batch
        processed = process_batch(chunk)

        # Forward to all downstream connections
        for conn in connections:
            try:
                send_chunk(conn, processed)
            except Exception as e:
                print(f"Error sending to downstream: {e}")

        batch_count += 1
        if batch_count % 10 == 0:
            print(f"Processed and forwarded {batch_count} batches")

    print(f"Finished processing {batch_count} batches")


def main():
    parser = argparse.ArgumentParser(description='Stream data processor')
    parser.add_argument('--listen-port', type=int, default=50001,
                       help='Port to listen for downstream connections')
    args = parser.parse_args()

    # Load peer discovery
    peers = load_peer_discovery()

    # Get upstream producer info
    upstream_stages = peers.get('upstream_stages', {})
    if not upstream_stages:
        print("Error: No upstream stages found")
        return 1

    # Get first upstream instance (for this example)
    first_upstream_stage = list(upstream_stages.keys())[0]
    upstream_instances = upstream_stages[first_upstream_stage]
    if not upstream_instances:
        print("Error: No upstream instances found")
        return 1

    upstream = upstream_instances[0]
    upstream_ip = upstream['private_ip']
    upstream_port = 50000  # Producer listens on 50000

    # Get downstream consumer info
    downstream_stages = peers.get('downstream_stages', {})
    total_consumers = sum(len(instances)
                         for instances in downstream_stages.values())

    print(f"Processor configuration:")
    print(f"  Upstream: {upstream_ip}:{upstream_port}")
    print(f"  Listening on port: {args.listen_port}")
    print(f"  Expected downstream consumers: {total_consumers}")

    # Create server for downstream connections
    server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    server_sock.bind(('0.0.0.0', args.listen_port))
    server_sock.listen(5)

    print(f"Waiting for {total_consumers} downstream connections...")

    # Accept downstream connections
    downstream_connections = []
    for i in range(total_consumers):
        conn, addr = server_sock.accept()
        print(f"Accepted connection {i+1}/{total_consumers} from {addr}")
        downstream_connections.append(conn)

    # Create queue for inter-thread communication
    data_queue = Queue(maxsize=10)

    # Start upstream receiver thread
    receiver_thread = threading.Thread(
        target=upstream_receiver,
        args=(upstream_ip, upstream_port, data_queue)
    )
    receiver_thread.start()

    # Start downstream forwarder thread
    forwarder_thread = threading.Thread(
        target=downstream_forwarder,
        args=(downstream_connections, data_queue)
    )
    forwarder_thread.start()

    # Wait for threads to complete
    receiver_thread.join()
    forwarder_thread.join()

    # Cleanup
    print("Closing downstream connections...")
    for conn in downstream_connections:
        conn.close()
    server_sock.close()

    print("Processor finished")
    return 0


if __name__ == '__main__':
    exit(main())
